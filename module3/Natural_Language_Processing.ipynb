{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":0: FutureWarning: IPython widgets are experimental and may change in the future.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "matplotlib.rcParams['savefig.dpi'] = 2 * matplotlib.rcParams['savefig.dpi']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word disambiguation using NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common NLP approaches\n",
    "It turns out that there are several common types of features and approaches that form the \"starting point\" for NLP and text-based data problems.  We'll talk about a few of the common ones:\n",
    "\n",
    "### Goals / Topics\n",
    " - Bag of words\n",
    " - TF/IDF\n",
    " - n-grams\n",
    " - Stemming / part of speech tagging / etc.\n",
    " - Feature hashing\n",
    " - Topics not covered\n",
    " \n",
    "Some useful tools:\n",
    " - http://scikit-learn.org/stable/modules/feature_extraction.html\n",
    " - http://www.nltk.org/\n",
    " - http://www.nltk.org/howto/wordnet.html\n",
    " \n",
    "If you're in a hurry just head there (with the caveat that nltk is huge).\n",
    "\n",
    "If you have more time, check out this comprehensive resource for using nltk with Python: http://www.nltk.org/book/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a good idea to start with a well-defined problem in mind.  So, let us pick:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a model for word disambiguation\n",
    "\n",
    "In a given source block of (prose) text, you want to be able to tell apart Apple (the company) vs. apple (the fruit).  Ideally you would also be able to tell apart Ford vs ford, Windows vs windows, etc. via similar examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**The type of learner**: We're going to choose to look at this as a _supervised classification_ problem.  There are also unsupervised approaches, but you have to make choices sometimes.  This means we need some \"marked up\" data:\n",
    "\n",
    "**The training dataset**: Having limited resources at our disposal, we're going to try to use Wikipedia's articles on the given topics as our chosen \"corpus\" of text.\n",
    "\n",
    "**The test dataset**: A good idea would be to mark up a small corpus by hand, or to use sentences culled from Wikipedia with the disambiguation coming from looking at the target of outgoing links.  We're not going to be that scientific for lack fo time.\n",
    "\n",
    "Okay, so you might be guessing that for \"Apple\"/\"apple\" simple heuristics (e.g., capitalization, presence of a possessive) would do quite well.  That is certainly true in this example!  To do better, you'd have to be more clever: An idea would be to look at nearby words (e.g., \"software\" and \"computer\" certainly hint one way, while \"flavor\" hints the other).  Figuring out how to turn this idea into an actual implementation takes us to our first two topics: **bag of words** and **TF/IDF**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature generation/extraction\n",
    "\n",
    "How might we do this?\n",
    "  1. We need to clean and tokenize the text data: __Tokenization__ refers to splitting the text into pieces, in this case into sentences and into words.  Cleaning can also include things like __stemming__ or __lemmatizing__ (identifying similar words like \"computer\" and \"computers\" to their stem).\n",
    "  2. We need to develop __features__.  We're going to think of our input as a sentence, and try to develop features of that sentence.  In our example application, we might try to use:\n",
    "   - Capitalized of the word apple? (_a_pple vs _A_pple)    \n",
    "   - Pluralization of the word apple? (apples)\n",
    "   - Possessive form of the word apple? (Apple's)\n",
    "   - Presence (or frequency) of certain well-chosen words : Does (e.g.,) the word \"computer\" or \"fruit\" occur in the sentence?  (This feature regards the sentence as a simple __bag of words__ without regard to trying to parse its structure.)\n",
    "   - In addition to single words, we can also look for __n-grams__: Strings of n consecutive words.\n",
    "   - There are common techniques for determining which words / n-grams to look for.  One of them is called __tf-idf__.\n",
    "  3. Finally, we'll run some sort of classifier on the features.\n",
    "  \n",
    "We'll mostly focus on general NLP techniques in 1 and 2, rather than diving deeply into techniques for word disambiguation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and tokenizing the data\n",
    "\n",
    "Our first step will be to pull in the training (and test) data.  We will want to clean both data on the way in: our goal is to have each text as a list of strings, one string for each sentence.\n",
    "\n",
    "We'll be using nltk already (to split things into sentences). We've already downloaded the nltk data to your box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Splitting into words/sentences:\n",
    "NLTK has convenient presets for sentence and word tokenization (i.e., splitting a document into sentences, resp. splitting a sentence into words).\n",
    ">        \n",
    "       my_list_of_sentences = nltk.tokenize.sent_tokenize(my_long_string) \n",
    "       words = [ nltk.tokenize.word_tokenize(sent) for sent in my_list_of_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashkan/anaconda/lib/python2.7/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 174 of the file /Users/ashkan/anaconda/lib/python2.7/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "import urllib2\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk.tokenize\n",
    "\n",
    "# Spit out (slightly cleaned up) sentences from a Wikipedia article.\n",
    "def wikipedia_to_sents(url):\n",
    "    soup = BeautifulSoup(urllib2.urlopen(url)).find(attrs={'id':'mw-content-text'})\n",
    "    \n",
    "    # The text is litered by references like [n].  Drop them.\n",
    "    def drop_refs(s):\n",
    "        return ''.join( re.split('\\[\\d+\\]', s) )\n",
    "    \n",
    "    paragraphs = [drop_refs(p.text) for p in soup.find_all('p')]\n",
    "    \n",
    "    raw_sents = reduce(lambda x, y: x + y, [nltk.tokenize.sent_tokenize(p.strip()) for p in paragraphs if p.strip()!=''])\n",
    "    return filter(lambda s: len(s.split(\" \"))>2, raw_sents)\n",
    "\n",
    "fruit_sents = wikipedia_to_sents(\"http://en.wikipedia.org/wiki/Apple\")\n",
    "company_sents = wikipedia_to_sents(\"http://en.wikipedia.org/wiki/Apple_Inc.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Apple immediately launched an investigation after the 2006 media report, and worked with their manufacturers to ensure acceptable working conditions.',\n",
       " u\"In 2007, Apple started yearly audits of all its suppliers regarding worker's rights, slowly raising standards and pruning suppliers that did not comply.\",\n",
       " u'Yearly progress reports have been published since 2008.',\n",
       " u\"In 2011, Apple admitted that its suppliers' child labor practices in China had worsened.\",\n",
       " u'The Foxconn suicides occurred between January and November 2010, when 18 Foxconn (Chinese: \\u5bcc\\u58eb\\u5eb7) employees attempted suicide, resulting in 14 deaths\\u2014the company was the world\\u2019s largest contract electronics manufacturer, for clients including Apple, at the time.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company_sents[-105:-100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u\"Some find them to have a better flavor than modern cultivars, but they may have other problems which make them commercially unviable from low yield, disease susceptibility, poor tolerance for storage or transport, or just being the 'wrong' size.\",\n",
       " u'A few old cultivars are still produced on a large scale, but many have been preserved by home gardeners and farmers that sell directly to local markets.',\n",
       " u'Many unusual and locally important cultivars with their own unique taste and appearance exist; apple conservation campaigns have sprung up around the world to preserve such local cultivars from extinction.',\n",
       " u\"In the United Kingdom, old cultivars such as 'Cox's Orange Pippin' and 'Egremont Russet' are still commercially important even though by modern standards they are low yielding and susceptible to disease.\",\n",
       " u'In the wild, apples grow readily from seeds.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fruit_sents[-105:-100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'However, like most perennial fruits, apples are ordinarily propagated asexually by grafting.',\n",
       " u'This is because seedling apples are an example of \"extreme heterozygotes\", in that rather than inheriting DNA from their parents to create a new apple with those characteristics, they are instead significantly different from their parents.',\n",
       " u'Triploid varieties have an additional reproductive barrier in that 3 sets of chromosomes cannot be divided evenly during meiosis, yielding unequal segregation of the chromosomes (aneuploids).',\n",
       " u'Even in the case when a triploid plant can produce a seed (apples are an example), it occurs infrequently, and seedlings rarely survive.',\n",
       " u'Because apples do not breed true when planted as seeds, grafting is generally used to produce new apple trees.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fruit_sents[-105:-100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text vectorization using \"bag of words\"\n",
    "\n",
    "Learning algorithms like vectors of numbers, not text.  The simplest way to turn a text into a vector of number is to treat the text as a \"bag of words.\"  That is you\n",
    "\n",
    "  - Split the text into words\n",
    "  - Count how many times each word (/each word in some fixed vocabulary) occurs\n",
    "  - _(Optionally)_ normalize the counts against some baseline\n",
    "  - _(Variant)_ Just do a binary \"yes / no\" for whether each word (/.. in some vocabulary) is contained in the material\n",
    "  \n",
    "The output is a very large, but usually sparse, vector: The number of coordinates is the number of words in our dictionary, and the $i$-th coordinate entry is the number of occurances of the $i$-th word.\n",
    "\n",
    "There's a reasonable implementation of this in the CountVectorizer class in sklearn.feature_extraction.text.  See http://scikit-learn.org/stable/modules/classes.html#text-feature-extraction-ref for more detail on the options.\n",
    "\n",
    "For instance, here we'll apply this to each sentence (as a separate bag)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(767, 3910)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bag_of_words_vectorizer = CountVectorizer()\n",
    "\n",
    "counts = bag_of_words_vectorizer.fit_transform( fruit_sents + company_sents  )\n",
    "print counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "  (0, 1529)\t1\n",
      "  (0, 2675)\t1\n",
      "  (0, 3454)\t1\n",
      "  (0, 1944)\t1\n",
      "  (0, 1489)\t1\n",
      "  (0, 2008)\t1\n",
      "  (0, 506)\t1\n",
      "  (0, 1400)\t1\n",
      "  (0, 3051)\t1\n",
      "  (0, 1794)\t1\n",
      "  (0, 1010)\t1\n",
      "  (0, 1927)\t1\n",
      "  (0, 1148)\t1\n",
      "  (0, 619)\t1\n",
      "  (0, 1298)\t1\n",
      "  (0, 335)\t1\n",
      "  (0, 802)\t1\n",
      "  (0, 2830)\t1\n",
      "  (0, 2168)\t2\n",
      "  (0, 3627)\t2\n",
      "  (0, 362)\t2\n",
      "  (0, 3529)\t3\n",
      "  (1, 1580)\t1\n",
      "  (1, 3305)\t1\n",
      "  (1, 1648)\t1\n",
      "  :\t:\n",
      "  (765, 628)\t1\n",
      "  (765, 2754)\t1\n",
      "  (765, 3438)\t1\n",
      "  (765, 374)\t1\n",
      "  (765, 852)\t1\n",
      "  (765, 3862)\t1\n",
      "  (765, 3592)\t1\n",
      "  (765, 2262)\t1\n",
      "  (765, 3527)\t1\n",
      "  (765, 2293)\t1\n",
      "  (765, 3576)\t1\n",
      "  (765, 1489)\t1\n",
      "  (765, 1794)\t1\n",
      "  (765, 335)\t1\n",
      "  (765, 362)\t1\n",
      "  (766, 1517)\t1\n",
      "  (766, 3881)\t1\n",
      "  (766, 3880)\t1\n",
      "  (766, 3086)\t1\n",
      "  (766, 84)\t1\n",
      "  (766, 3003)\t1\n",
      "  (766, 411)\t1\n",
      "  (766, 341)\t1\n",
      "  (766, 1794)\t1\n",
      "  (766, 3529)\t1\n"
     ]
    }
   ],
   "source": [
    "# Note that counts is a **sparse** matrix.\n",
    "print counts.toarray() #This is what it actually looks like.. there are non-zero entries, really!\n",
    "print counts           # .. this is just describing the non-zero entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'deals', u'death', u'deaths', u'debated', u'debt']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words_vectorizer.get_feature_names()[1000:1005]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop words\n",
    "It's common to want to __omit__ certain common words when doing these counts -- \"a\", \"an\", and \"the\" are common enough so that their counts do not tend to give us any hints as to the meaning of documents.  Such words that we want to omit are called __stop words__ (they don't stop anything, though).\n",
    "\n",
    "NLTK contains a standard list of such stop words for English in `nltk.corpus.stopwords.words('english')`.  In our application, we'd also want to include \"apple\" -- it is certainly not going to help us distinguish our two meanings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'i',\n",
       " u'me',\n",
       " u'my',\n",
       " u'myself',\n",
       " u'we',\n",
       " u'our',\n",
       " u'ours',\n",
       " u'ourselves',\n",
       " u'you',\n",
       " u'your',\n",
       " u'yours',\n",
       " u'yourself',\n",
       " u'yourselves',\n",
       " u'he',\n",
       " u'him',\n",
       " u'his',\n",
       " u'himself',\n",
       " u'she',\n",
       " u'her',\n",
       " u'hers',\n",
       " u'herself',\n",
       " u'it',\n",
       " u'its',\n",
       " u'itself',\n",
       " u'they',\n",
       " u'them',\n",
       " u'their',\n",
       " u'theirs',\n",
       " u'themselves',\n",
       " u'what',\n",
       " u'which',\n",
       " u'who',\n",
       " u'whom',\n",
       " u'this',\n",
       " u'that',\n",
       " u'these',\n",
       " u'those',\n",
       " u'am',\n",
       " u'is',\n",
       " u'are',\n",
       " u'was',\n",
       " u'were',\n",
       " u'be',\n",
       " u'been',\n",
       " u'being',\n",
       " u'have',\n",
       " u'has',\n",
       " u'had',\n",
       " u'having',\n",
       " u'do',\n",
       " u'does',\n",
       " u'did',\n",
       " u'doing',\n",
       " u'a',\n",
       " u'an',\n",
       " u'the',\n",
       " u'and',\n",
       " u'but',\n",
       " u'if',\n",
       " u'or',\n",
       " u'because',\n",
       " u'as',\n",
       " u'until',\n",
       " u'while',\n",
       " u'of',\n",
       " u'at',\n",
       " u'by',\n",
       " u'for',\n",
       " u'with',\n",
       " u'about',\n",
       " u'against',\n",
       " u'between',\n",
       " u'into',\n",
       " u'through',\n",
       " u'during',\n",
       " u'before',\n",
       " u'after',\n",
       " u'above',\n",
       " u'below',\n",
       " u'to',\n",
       " u'from',\n",
       " u'up',\n",
       " u'down',\n",
       " u'in',\n",
       " u'out',\n",
       " u'on',\n",
       " u'off',\n",
       " u'over',\n",
       " u'under',\n",
       " u'again',\n",
       " u'further',\n",
       " u'then',\n",
       " u'once',\n",
       " u'here',\n",
       " u'there',\n",
       " u'when',\n",
       " u'where',\n",
       " u'why',\n",
       " u'how',\n",
       " u'all',\n",
       " u'any',\n",
       " u'both',\n",
       " u'each',\n",
       " u'few',\n",
       " u'more',\n",
       " u'most',\n",
       " u'other',\n",
       " u'some',\n",
       " u'such',\n",
       " u'no',\n",
       " u'nor',\n",
       " u'not',\n",
       " u'only',\n",
       " u'own',\n",
       " u'same',\n",
       " u'so',\n",
       " u'than',\n",
       " u'too',\n",
       " u'very',\n",
       " u's',\n",
       " u't',\n",
       " u'can',\n",
       " u'will',\n",
       " u'just',\n",
       " u'don',\n",
       " u'should',\n",
       " u'now',\n",
       " u'd',\n",
       " u'll',\n",
       " u'm',\n",
       " u'o',\n",
       " u're',\n",
       " u've',\n",
       " u'y',\n",
       " u'ain',\n",
       " u'aren',\n",
       " u'couldn',\n",
       " u'didn',\n",
       " u'doesn',\n",
       " u'hadn',\n",
       " u'hasn',\n",
       " u'haven',\n",
       " u'isn',\n",
       " u'ma',\n",
       " u'mightn',\n",
       " u'mustn',\n",
       " u'needn',\n",
       " u'shan',\n",
       " u'shouldn',\n",
       " u'wasn',\n",
       " u'weren',\n",
       " u'won',\n",
       " u'wouldn']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'000', u'10', u'100', u'12', u'14', u'16', u'19', u'20', u'2006', u'2007', u'2008', u'2009', u'2010', u'2011', u'2012', u'2013', u'2014', u'2015', u'2016', u'22', u'24', u'30', u'500', u'800', u'access', u'according', u'added', u'also', u'america', u'american', u'announced', u'app', u'apples', u'applications', u'apps', u'april', u'around', u'august', u'available', u'average', u'back', u'based', u'became', u'began', u'best', u'billion', u'board', u'brand', u'business', u'california', u'called', u'camera', u'campus', u'century', u'ceo', u'china', u'city', u'climate', u'commercial', u'companies', u'company', u'computer', u'computers', u'conditions', u'consumer', u'consumers', u'cook', u'corporation', u'countries', u'cultivars', u'data', u'davidson', u'day', u'days', u'december', u'design', u'designed', u'desktop', u'developed', u'development', u'device', u'different', u'digital', u'directly', u'disease', u'display', u'due', u'early', u'electronics', u'employees', u'energy', u'europe', u'even', u'every', u'featured', u'features', u'february', u'first', u'following', u'found', u'fourth', u'foxconn', u'free', u'fruit', u'gb', u'generally', u'generation', u'global', u'golden', u'grown', u'hardware', u'help', u'high', u'higher', u'ii', u'imac', u'important', u'improved', u'inc', u'inch', u'include', u'includes', u'including', u'industry', u'information', u'internal', u'internet', u'introduced', u'ios', u'ipad', u'iphone', u'ipod', u'itunes', u'january', u'jobs', u'july', u'june', u'known', u'labor', u'large', u'largest', u'last', u'late', u'later', u'launch', u'launched', u'led', u'life', u'like', u'line', u'logo', u'long', u'low', u'mac', u'macintosh', u'made', u'major', u'make', u'malus', u'management', u'many', u'march', u'market', u'may', u'media', u'microsoft', u'million', u'mini', u'models', u'modern', u'much', u'music', u'name', u'new', u'north', u'november', u'number', u'numerous', u'october', u'often', u'one', u'online', u'opening', u'operating', u'operations', u'original', u'os', u'outside', u'pay', u'people', u'per', u'period', u'personal', u'platform', u'points', u'policy', u'power', u'practices', u'president', u'price', u'pro', u'processor', u'produce', u'produced', u'producing', u'product', u'production', u'products', u'profit', u'profits', u'program', u'project', u'provided', u'public', u'published', u'quarter', u'range', u'rate', u'record', u'release', u'released', u'renewable', u'replaced', u'report', u'reported', u'research', u'retail', u'revenue', u'rootstocks', u'sales', u'samsung', u'sculley', u'second', u'seeds', u'sell', u'september', u'series', u'service', u'several', u'share', u'significant', u'since', u'size', u'small', u'smartphone', u'software', u'sold', u'square', u'states', u'steve', u'still', u'stock', u'storage', u'store', u'stores', u'success', u'system', u'take', u'tax', u'team', u'technology', u'third', u'though', u'three', u'time', u'top', u'total', u'touch', u'tree', u'trees', u'tv', u'two', u'uk', u'united', u'unveiled', u'update', u'us', u'use', u'used', u'user', u'users', u'using', u'varieties', u'variety', u'version', u'vice', u'video', u'watch', u'well', u'within', u'without', u'work', u'workers', u'working', u'world', u'worldwide', u'would', u'wozniak', u'year', u'years']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<201x300 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 676 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The vocabulary *can* be built for you.  \n",
    "#\n",
    "# For instance, here we'll compute and then use the top 100 words by frequency -- *ignoring*\n",
    "# the so-calle \"stopwords\": these are words like \"a\", \"and\", \"the\" that are very common\n",
    "# \"apple\" is not useful for distinguishing the two, but is common, so add it as a stopword.\n",
    "#\n",
    "# Nevertheless, this method is probably NOT GOOD.  See tf-idf below instead.\n",
    "counter=CountVectorizer(max_features=300,\n",
    "                        stop_words=nltk.corpus.stopwords.words('english') + ['apple'])\n",
    "counter=counter.fit( fruit_sents + company_sents )\n",
    "print counter.get_feature_names()\n",
    "\n",
    "# Now we can use it with that vectorizer, like so...\n",
    "counter.transform(company_sents)\n",
    "counter.transform(fruit_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams\n",
    "\n",
    "Instead of looking at just single words, it is also useful to look at **n-grams**: These are n-word long sequences of words (i.e., each of \"farmer's market\", \"market share\", and \"farm share\" is a 2-gram).\n",
    "\n",
    "The exact same sort of counting techniques apply.  The `CountVectorizer` function has built in support for this, too:\n",
    "\n",
    "If you pass it the `ngram_range=(m, M)` then it will count $n$-grams with  $m \\leq n \\leq M$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'000 square', u'100 renewable', u'19th century', u'2011 jobs', u'2012 update', u'2014 update', u'2015 update', u'2016 update', u'21 2016', u'40 gb', u'800 000', u'a4 processor', u'according report', u'added support', u'adverse reactions', u'ago quarter', u'aid ireland', u'aim alliance', u'alexander great', u'allergy also', u'also develop', u'also include', u'also includes', u'also introduced', u'also known', u'also led', u'also made', u'also offers', u'also used', u'ancestor malus', u'announced march', u'announced would', u'annual best', u'annual revenue', u'app store', u'apples 2013', u'apples also', u'apples commonly', u'apples example', u'apples generally', u'apples religious', u'apps earth', u'april 24', u'backlit lcd', u'became chairman', u'became first', u'become world', u'begin producing', u'best global', u'billion cash', u'billion downloads', u'billion euros', u'billion plus', u'birch pollen', u'birch syndrome', u'board directors', u'brand loyalty', u'brands report', u'brought back', u'bud sports', u'carbon dioxide', u'carbon footprint', u'central asia', u'ceo michael', u'child labor', u'cider north', u'citation needed', u'climate counts', u'coca cola', u'collection database', u'commercial orchards', u'commissioner concluded', u'commonly used', u'company history', u'company name', u'company product', u'company revenue', u'company sold', u'competition commissioner', u'computer inc', u'computer software', u'computer sold', u'computers use', u'concluded received', u'consumer electronics', u'consumer market', u'control size', u'controlled atmosphere', u'corporate culture', u'corporation world', u'cultivars vary', u'cut pro', u'desktop publishing', u'devices running', u'didi chuxing', u'different cultivars', u'dr dre', u'dwarf rootstocks', u'fifth avenue', u'final cut', u'first three', u'first time', u'fiscal year', u'forbidden fruit', u'form allergy', u'fourth generation', u'full time', u'generation ipad', u'global brands', u'golden apples', u'high level', u'inch display', u'inch screen', u'intellectual property', u'internal memo', u'introduced iphone', u'introduced new', u'ipad mini', u'iphone 3gs', u'iphone 4s', u'iphone ipod', u'iphone plus', u'iphone released', u'iphone tv', u'ipod portable', u'ipod touch', u'ireland eu', u'itunes library', u'itunes radio', u'itunes store', u'january 10', u'january 1977', u'january 2007', u'january 27', u'japanese market', u'jef raskin', u'jobs announced', u'jobs back', u'jobs immediately', u'jobs resigned', u'jobs several', u'jobs stated', u'jobs steve', u'jobs would', u'john sculley', u'jonathan ive', u'july 2013', u'july 2015', u'june 2009', u'june 2016', u'key grip', u'keynote address', u'known cultivars', u'labor practices', u'labor violations', u'lal shimpi', u'largest mobile', u'largest music', u'largest publicly', u'late 1990s', u'later versions', u'later year', u'latter world', u'launch ipad', u'lcd displays', u'leave absence', u'level brand', u'life threatening', u'lisa team', u'list companies', u'logo designed', u'low tax', u'lower cost', u'mac os', u'macbook pro', u'macworld expo', u'malus sieversii', u'mapping company', u'march 2016', u'march 21', u'market capitalization', u'market share', u'may also', u'media player', u'million year', u'new campus', u'new york', u'norse mythology', u'north america', u'north carolina', u'northern europe', u'november 10', u'october 23', u'often eaten', u'old cultivars', u'online services', u'online store', u'operating system', u'operations run', u'os new', u'per share', u'personal computer', u'personal computers', u'price points', u'product line', u'profit margins', u'reduced price', u'renewable energy', u'retail stores', u'retina display', u'revenue generation', u'revenue world', u'right policy', u'ronald wayne', u'rootstocks used', u'run renewable', u'running ios', u'safari web', u'sales international', u'samsung electronics', u'second largest', u'second quarter', u'selective breeding', u'sell personal', u'sell third', u'senior vice', u'september 2012', u'september 2014', u'september 2015', u'september 2016', u'set new', u'seventeen countries', u'share price', u'shares hit', u'silicon valley', u'size tree', u'small amounts', u'software includes', u'software titles', u'software update', u'solar energy', u'sold first', u'sold million', u'split adjusted', u'square foot', u'stainless steel', u'state aid', u'steve jobs', u'steve wozniak', u'stock price', u'storage syncing', u'store itunes', u'store latter', u'store new', u'store surpassed', u'stores seventeen', u'streaming itunes', u'super bowl', u'supply chain', u'surpassed size', u'technology company', u'third fourth', u'third generation', u'third party', u'thousands years', u'three days', u'three year', u'tim cook', u'time employees', u'time high', u'time quarterly', u'tonnes china', u'total united', u'trade shows', u'traded corporation', u'tree even', u'trees large', u'tv runs', u'two companies', u'two stores', u'uk national', u'ultimate size', u'united kingdom', u'united states', u'unpaid taxes', u'update 100', u'update maintains', u'us billion', u'used iphone', u'used produce', u'user interface', u'vice president', u'wi fi', u'wide range', u'wild ancestor', u'working conditions', u'world largest', u'year ago', u'year introduced', u'york city']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<201x300 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 139 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ng_counter=CountVectorizer(max_features=300, \n",
    "                           ngram_range=(2,2), \n",
    "                           stop_words=nltk.corpus.stopwords.words('english') + ['apple', 'Apple'])\n",
    "ng_counter=ng_counter.fit( fruit_sents + company_sents  )\n",
    "print ng_counter.get_feature_names()\n",
    "\n",
    "# Now we can use it with that vectorizer, like so...\n",
    "ng_counter.transform(company_sents)\n",
    "ng_counter.transform(fruit_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term frequency–inverse document frequency (TF-IDF)\n",
    "\n",
    "With single word vocabularies, we can probably do an okay job of coming up with a reasonable (if short) list of words that distinguish between the two documents.  With n-grams, even for $n=2$, it is better to let a computer help us.  \n",
    "\n",
    "Just using frequencies, as above, is clearly not great.  Both apples the fruit and Apple the company are enjoyed around the world (one of the 2-grams that came up above!).  We would like to find words that are common in one document, not not common in all of them.  This is the goal of the __tf-idf weighting__.  A precise definition is:\n",
    "\n",
    "\n",
    "  1. If $d$ denotes a document and $t$ denotes a term, then the _raw term frequency_ $\\mathrm{tf}^{raw}(t,d)$ is\n",
    "  $$ \\mathrm{tf}^{raw}(t,d) = \\text{the number of times the term $t$ occurs in the document $d$} $$\n",
    "  The vector of all term frequencies can optionally be _normalized_ either by dividing by the maximum of ny single word's occurance count ($L^1$) or by the Euclidean length of the vector of word occurance counts ($L^2$).  Scikit-learn by defaults does this second one:\n",
    "  $$ \\mathrm{tf}(t,d) = \\mathrm{tf}^{L^2}(t,d) = \\frac{\\mathrm{tf}^{raw}(t,d)}{\\sqrt{\\sum_t \\mathrm{tf}^{raw}(t,d)^2}} $$\n",
    "  2. If $$ D = \\left\\{ d : d \\in D \\right\\} $$ is the set of possible documents, then  the _inverse document frequency_ is\n",
    "  $$ \\mathrm{idf}^{naive}(t,D) = \\log \\frac{\\# D}{\\# \\{d \\in D : t \\in d\\}} \\\\\n",
    "  = \\log \\frac{\\text{count of all documents}}{\\text{count of those documents containing the term $t$}} $$\n",
    "  with a common variant being\n",
    "  $$ \\mathrm{idf}(t, D) = \\log \\frac{\\# D}{1 + \\# \\{d \\in D : t \\in d\\}} \\\\\n",
    "   = \\log \\frac{\\text{count of all documents}}{1 + \\text{count of those documents containing the term $t$}} $$\n",
    "  (This second one is the default in scikit-learn. Without this tweak we would omit the $1+$ in the denominator and have to worry about dividing by zero if $t$ is not found in any documents.)\n",
    "  3. Finally, the weight that we assign to the term $t$ appearing in document $d$ and depending on the corpus of all documents $D$ is\n",
    "  $$ \\mathrm{tfidf}(t,d,D) = \\mathrm{tf}(t,d) \\mathrm{idf}(t,D) $$\n",
    "  \n",
    "  \n",
    "  \n",
    "###Exercises:\n",
    "  1. Imagine that $D$ consists of just two documents $D = \\{ d_1, d_2 \\}$ and that the word \"cultivar\" occurs in $d_1$ but not in $d_2$.  What is \n",
    "  $$ \\mathrm{tfidf}(\\mathrm{\"cultivar\"}, d_i, D)$$\n",
    "for each $i=1,2$?  For simplicity, use $\\mathrm{tf}^{raw}$ and the version of `idf` without the $1+$.  \n",
    "\n",
    "  2. Same question as 1, but now use $\\mathrm{tf}^{L^2}$ and the version of of `idf` with the $1+$.  \n",
    "\n",
    "  3. What happens to the tf-idf weighting of a word if it occurs in all (or all but one) documents?  Consider both forms of `idf`.\n",
    "  \n",
    "  4. In the example below, we consider each sentence as a separate document for the purpose of tf-idf.  What happens if you instead treat the input as just two documents, one for each starting article.\n",
    "  \n",
    "### Hints/Answers:\n",
    "  1. For $i=2$ it is zero.  For $i=1$, it is the number of occurances of \"apple\" in $d_1$ multiplied by $\\log 2$.\n",
    "  2. For $i=2$, it is zero.  For $i=1$, it is .. also zero.\n",
    "  3. Answer: If it occurs in all documents, and there are $N$ of the, then the $1+$ form weights `idf` by $\\log N/(N+1) < 0$ while the other form weights `idf` by $\\log N/N = 0$.  If it occurs in all-but-one document, then th $1+$ form weights `idf` by $0$ while the other form weights it by $\\log N/(N-1) \\approx 1+1/N$.\n",
    "  4. It works less well, because of what's discussed in 3.  tf-idf doesn't work so well with few documents and where relevant words  occur (even if with wildly different frequencies!) in both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'free', u'fruit', u'gb', u'generally', u'generation']\n",
      "  (0, 266)\t0.555466017677\n",
      "  (0, 157)\t0.616736422229\n",
      "  (0, 135)\t0.281233100567\n",
      "  (0, 101)\t0.24304803174\n",
      "  (0, 49)\t0.284959156701\n",
      "  (0, 43)\t0.30286558921\n",
      "  (1, 295)\t0.44924812985\n",
      "  (1, 266)\t0.43181978017\n",
      "  (1, 157)\t0.479451447602\n",
      "  (1, 107)\t0.488909099042\n",
      "  (1, 101)\t0.377891515579\n",
      "  (2, 266)\t0.468915371239\n",
      "  (2, 248)\t0.520638849511\n",
      "  (2, 157)\t0.520638849511\n",
      "  (2, 97)\t0.487840907854\n",
      "  (3, 299)\t0.408158116811\n",
      "  (3, 170)\t0.437710543153\n",
      "  (3, 107)\t0.462119213432\n",
      "  (3, 89)\t0.453179796161\n",
      "  (3, 28)\t0.472112631712\n",
      "  (4, 158)\t0.74917106487\n",
      "  (4, 120)\t0.662376566284\n",
      "  (5, 267)\t0.541535191874\n",
      "  (5, 137)\t0.583190153059\n",
      "  (5, 107)\t0.605498869806\n",
      "  :\t:\n",
      "  (762, 109)\t0.426696935931\n",
      "  (762, 35)\t0.36889463683\n",
      "  (762, 30)\t0.339320463324\n",
      "  (762, 18)\t0.320390224735\n",
      "  (762, 4)\t0.426696935931\n",
      "  (763, 251)\t0.345230904144\n",
      "  (763, 216)\t0.387104491108\n",
      "  (763, 129)\t0.395208388905\n",
      "  (763, 34)\t0.454144317061\n",
      "  (763, 32)\t0.438642140216\n",
      "  (763, 31)\t0.419669009144\n",
      "  (764, 251)\t0.243834086708\n",
      "  (764, 35)\t0.288892677284\n",
      "  (764, 34)\t0.320758841272\n",
      "  (764, 33)\t0.334159426304\n",
      "  (764, 32)\t0.309809766066\n",
      "  (764, 31)\t0.592818362066\n",
      "  (764, 20)\t0.334159426304\n",
      "  (764, 2)\t0.288892677284\n",
      "  (765, 291)\t0.527743177993\n",
      "  (765, 264)\t0.501820008961\n",
      "  (765, 164)\t0.437208136671\n",
      "  (765, 34)\t0.527743177993\n",
      "  (766, 30)\t0.727097758595\n",
      "  (766, 18)\t0.686533939034\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "ng_tfidf=TfidfVectorizer(max_features=300, \n",
    "                         ngram_range=(1,2), \n",
    "                         stop_words=nltk.corpus.stopwords.words('english') + [\"apple\", \"apples\"])\n",
    "ng_tfidf=ng_tfidf.fit( fruit_sents + company_sents )\n",
    "print ng_tfidf.get_feature_names()[100:105]\n",
    "print ng_tfidf.transform( fruit_sents + company_sents )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document similarity metrics\n",
    "\n",
    "A common problem is looking up a document similar to a given snippet, or relatedly comparing two documents for similarity.  The above provides a simple method for this called __cosine similarity__:\n",
    "  - To each of the two douments $d_1, d_2$ in a corpus of documents $D$, assign its tf or tf-idf vector $$ (v_i)_{j} = \\mathrm{tfidf}( t_{j}, d_i, D ) $$\n",
    "  where $i$ ranges over indices for documents, and $j$ ranges over indices for terms in the vocabulary.\n",
    "  - To compare two documents, simply find the cosine of the angle between the vectors:\n",
    "  $$ \\frac{v_i \\cdot v_{i'}}{|v_i| |v_{i'}|} $$\n",
    "  \n",
    "(There's also a variant using binary vectors and Jaccard distance.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "In our original hand-built vocabulary, we had to include both \"computer\" and \"computers\".  It would have been useful to identify them as one word.\n",
    "\n",
    "This is not limited to just trailing \"s\" characters: e.g., the words \"carry\", \"carries\", \"carrying\", and \"carried\" all carry -- roughly -- the same meaning.  The process of replacing them by a common root, or **stem**, is called stemming -- the stem will not, in general, be a full word itself.\n",
    "\n",
    "There's a related process called **lemmatization**: The analog of the \"stem\" here _is_ an actual word.  We can choose to first stem our words before counting them:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stemming carry [u'carri', u'carri', u'carri', u'carri']\n",
      "stemming eat [u'eat', u'eat', u'eaten', u'ate']\n",
      "the quick brown fox jumped over the lazy dog.  i can't believe it's not butter.  i tried to ford the river and my unfortunate oxen died.\n",
      "the quick brown fox jump over the lazi dog.  i can't believ it not butter.  i tri to ford the river and my unfortun oxen died.\n"
     ]
    }
   ],
   "source": [
    "stemmer = nltk.stem.SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "print \"stemming carry\", [stemmer.stem(s) for s in [\"carry\", \"carries\", \"carrying\", \"carried\"]]\n",
    "print \"stemming eat\", [stemmer.stem(s) for s in [\"eat\", \"eating\", \"eaten\", \"ate\"]]\n",
    "                                \n",
    "# More examples \n",
    "print stemmer.stem(\"The quick brown fox jumped over the lazy dog.  I can't believe it's not butter.  I tried to ford the river and my unfortunate oxen died.\")\n",
    "print \" \".join(map(stemmer.stem, \"The quick brown fox jumped over the lazy dog.  I can't believe it's not butter.  I tried to ford the river and my unfortunate oxen died.\".split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma carry ['carry', u'carry', 'carrying', 'carried']\n",
      "lemma eat ['eat', 'eating', 'eaten', 'ate']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "print \"lemma carry\", [lemmatizer.lemmatize(s) for s in [\"carry\", \"carries\", \"carrying\", \"carried\"]]\n",
    "print \"lemma eat\", [lemmatizer.lemmatize(s) for s in [\"eat\", \"eating\", \"eaten\", \"ate\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can tell our bag-of-words counters (/tf-idf) to first run its input through the stemmer.  This way it won't have to include both e.g., 'computer' and 'computers':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'000', u'10', u'100', u'12', u'2006', u'2007', u'2008', u'2009', u'2010', u'2011', u'2012', u'2013', u'2014', u'2015', u'2016', u'30', u'500', u'access', u'accord', u'ad', u'addit', u'allow', u'also', u'american', u'announc', u'app', u'app store', u'applic', u'april', u'around', u'august', u'avail', u'back', u'base', u'becam', u'began', u'best', u'billion', u'board', u'brand', u'build', u'busi', u'call', u'camera', u'campus', u'center', u'centuri', u'ceo', u'chang', u'china', u'citi', u'color', u'commerci', u'common', u'compani', u'comput', u'consum', u'continu', u'control', u'cook', u'corpor', u'countri', u'creat', u'cultiv', u'cultivar', u'current', u'data', u'day', u'decemb', u'design', u'desktop', u'develop', u'devic', u'differ', u'digit', u'direct', u'diseas', u'display', u'download', u'due', u'earli', u'effect', u'electron', u'employe', u'end', u'energi', u'engin', u'europ', u'even', u'event', u'everi', u'facil', u'factori', u'featur', u'first', u'focus', u'follow', u'form', u'found', u'foxconn', u'free', u'fresh', u'fruit', u'gb', u'general', u'generat', u'global', u'golden', u'grown', u'hardwar', u'help', u'high', u'ii', u'imac', u'improv', u'inc', u'inch', u'includ', u'increas', u'individu', u'industri', u'inform', u'intern', u'introduc', u'io', u'ipad', u'iphon', u'ipod', u'issu', u'itun', u'itun store', u'januari', u'job', u'juli', u'june', u'known', u'larg', u'largest', u'last', u'later', u'launch', u'leav', u'led', u'life', u'like', u'line', u'locat', u'logo', u'long', u'low', u'mac', u'mac os', u'macintosh', u'made', u'major', u'make', u'malus', u'manag', u'mani', u'manufactur', u'march', u'market', u'may', u'media', u'microsoft', u'million', u'model', u'much', u'music', u'name', u'need', u'new', u'north', u'novemb', u'number', u'octob', u'offer', u'offic', u'often', u'one', u'onlin', u'open', u'oper', u'oper system', u'organ', u'origin', u'os', u'particular', u'pay', u'peopl', u'per', u'period', u'person', u'person comput', u'pest', u'phone', u'plant', u'platform', u'player', u'point', u'popular', u'power', u'practic', u'present', u'presid', u'price', u'pro', u'processor', u'produc', u'product', u'profit', u'program', u'project', u'provid', u'public', u'publish', u'purchas', u'quarter', u'rang', u'rank', u'rate', u'reaction', u'receiv', u'record', u'releas', u'renew', u'renew energi', u'replac', u'report', u'research', u'result', u'retail', u'retail store', u'revenu', u'rootstock', u'run', u'sale', u'sculley', u'seed', u'select', u'sell', u'septemb', u'seri', u'servic', u'set', u'sever', u'share', u'show', u'signific', u'similar', u'sinc', u'size', u'smartphon', u'softwar', u'sold', u'sourc', u'specif', u'standard', u'state', u'steve', u'still', u'stock', u'storag', u'store', u'success', u'suicid', u'supplier', u'system', u'take', u'tax', u'technolog', u'third', u'three', u'time', u'top', u'total', u'touch', u'tree', u'tv', u'two', u'unit', u'unit state', u'updat', u'us', u'use', u'user', u'varieti', u'version', u'video', u'well', u'wide', u'within', u'without', u'work', u'worker', u'world', u'worldwid', u'would', u'year', u'yield']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "default_tokenizer = TfidfVectorizer().build_tokenizer()\n",
    "stemmer = nltk.stem.SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "    \n",
    "def tokenize_stem(text):\n",
    "    \"\"\"\n",
    "    We will use the default tokenizer from TfidfVectorizer, combined with the nltk SnowballStemmer.\n",
    "    \"\"\"\n",
    "    tokens = default_tokenizer(text)\n",
    "    stemmed = map(stemmer.stem, tokens)\n",
    "    return stemmed\n",
    "\n",
    "ng_stem_tfidf = TfidfVectorizer(max_features=300, \n",
    "                         ngram_range=(1,2), \n",
    "                         stop_words=map(stemmer.stem, nltk.corpus.stopwords.words('english') + [\"apple\"]),\n",
    "                         tokenizer=tokenize_stem)\n",
    "ng_stem_tfidf = ng_stem_tfidf.fit( fruit_sents + company_sents )\n",
    "\n",
    "ng_stem_vocab = ng_stem_tfidf.get_feature_names()\n",
    "print ng_stem_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization using feature hashing\n",
    "\n",
    "When doing \"bag of words\" type techniques on a *large* corpus and without an existing vocabulary, there is a simple trick that is often useful.  The issue (and solution) is as follows: \n",
    "\n",
    " - The output is a feature vector, so that whenever we encounter a word we must look up which coordinate slot it is in.  A naive way would be to keep a list of all the words encoutered so far, and look up each word when it is encountered.  Whenever we encounter a new word, we see if we've already seen it before and if not -- assign it a new number.  This requires storing all the words that we have seen in memory, cannot be done in parallel (because we'd have to share the hash-table of seen words), etc.\n",
    " - A **hash function** takes as input something complicated (like a string) and spits out a number, with the desired property being that different inputs *usually* produce different outputs.  (This is how hash tables are implemented, as the name suggests.)\n",
    " - So -- rather than exactly looking up the coordinate of a given word, we can just use its hash value (modulo a big size that we choose).  This is fast and parallelizes easily.  (There are some downsides: You cannot tell, after the fact, what word each of your feature actually corresponds to!)\n",
    " \n",
    "Scikit-learn includes `sklearn.feature_extraction.text.HashingVectorizer` to do this.  It behaves as almost a drop-in replacement for `CountVectorizer`.  It can be used with tf-idf by combining it with the `TfidfTransformer` (the `TfidfVectorizer` is the `CountVectorizer` together with the `TfidfTransformer`). For our application (where the training and test data is small), we may as well just use `TfidfVectorizer` -- but it is good to know that `HashingVectorizer` is there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating additional features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of speech tagging\n",
    "Consider the \"Ford\" vs \"ford\" example.  As a human being, the easiest way to tell these apart is that Ford is a __noun__ while ford is a __verb__.\n",
    "\n",
    "Fortunately, NLTK also has a part-of-speech tagger: You give it a sentence, and it tries to tag the parts of speech (e.g., noun, verb, adjective, etc.).  The command is `nltk.pos_tag` and for documentation on the tags either search around online, or use `nltk.help.upenn_tagset`:\n",
    "\n",
    "(N.B. Nothing's perfect -- the tagger makes mistakes in each of the examples below!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s1 = \"I tried to ford the river, and my unfortunate oxen died\"\n",
    "s2 = \"Henry Ford built factories to facilitate the construction of the Ford automobile.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nltk.pos_tag(nltk.tokenize.word_tokenize(s1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nltk.pos_tag(nltk.tokenize.word_tokenize(s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nltk.help.upenn_tagset('NN.*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capitalization, punctuation, etc.\n",
    "There are the obvious features that we had in mind...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def feature_verbs(words, positions):\n",
    "    pos_tag = nltk.pos_tag(words)\n",
    "    return len( [ i for i in positions if pos_tag[i][1] == 'VB'] )\n",
    "\n",
    "def is_cap(word):\n",
    "    return word[0] == word[0].capitalize()\n",
    "\n",
    "def feature_caps(words, positions):\n",
    "    return len ( [ i for i in positions if is_cap(words[i]) ] )\n",
    "\n",
    "def feature_plural(words, positions):\n",
    "    def is_plural(word):\n",
    "        return re.match( \".*s$\", word )\n",
    "    return len ( [ i for i in positions if is_plural(words[i]) ] )\n",
    "\n",
    "## N.B. The nltk word tokenizer will tokenize \"Apple's\" as [\"Apple\", \"'s\"]\n",
    "def feature_posessive(words, positions):\n",
    "    l = len(words)\n",
    "    return len ( [ i for i in positions if i+1 < l and words[i+1]==\"'s\" ] )\n",
    "\n",
    "def ad_hoc_features(keyword, strs):\n",
    "    \"\"\"\n",
    "    Given a keyword (e.g., \"apple\") and a list of strings;\n",
    "    Returns a numpy ndarray encoding several ad hoc features of the string that are local\n",
    "    near occurances of the keyword:\n",
    "        - If the keyword is capitalized\n",
    "        - If it is plural (in the stupid sense of ending in s.. good enough for 'apple')\n",
    "        - If it is possessive in the stupid sense of being followed by 's)\n",
    "        - If the keyword is a verb (e.g., for Ford vs ford)\n",
    "    \"\"\"\n",
    "    stemmed_word = stemmer.stem(keyword)\n",
    "    def feature_one(s):\n",
    "        words = nltk.tokenize.word_tokenize(s)\n",
    "        hits = [ i for i in range(len(words)) if stemmer.stem(words[i]) == stemmed_word ]\n",
    "        return np.array([ feature_caps(words, hits), \n",
    "                          feature_plural(words, hits), \n",
    "                          feature_posessive(words, hits), \n",
    "                          feature_verbs(words, hits) ])\n",
    "    return np.asarray(map(feature_one, strs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ad_hoc_features(\"ford\", [\"I drive a Ford.\", \"I tried to ford the river.\", \"That's not Ford's.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ad_hoc_features(\"apple\", [\"Have you eaten your apple?\", \"How is Apple's stock doing?\", \"Apples are tasty.\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining ideas into an application\n",
    "Diclaimer: This version is actually pretty bad -- it uses many of the right ideas, but them them together pretty poorly (and with fairly little available data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "import re\n",
    "import nltk.tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "def wikipedia_to_sents(url):\n",
    "    \"\"\"\n",
    "    Retrieves a URL from wikipedia, and returns a list of sentences (of at least 3 words) in the body text.\n",
    "    \"\"\"\n",
    "    files_by_url = {\n",
    "      \"http://en.wikipedia.org/wiki/Ford_(crossing)\": \"ford_crossing.txt\",\n",
    "      \"http://en.wikipedia.org/wiki/Ford\": \"ford_car.txt\",\n",
    "      \"http://en.wikipedia.org/wiki/Apple\": \"apple_fruit.txt\",\n",
    "      \"http://en.wikipedia.org/wiki/Apple_Inc.\": \"apple_inc.txt\",\n",
    "      \"http://en.wikipedia.org/wiki/Window\": \"window_glass.txt\",\n",
    "      \"http://en.wikipedia.org/wiki/Microsoft_Windows\": \"windows_ms.txt\"\n",
    "    }\n",
    "    \n",
    "    with open(\"small_data/{}\".format(files_by_url[url])) as wiki_file:\n",
    "        soup = BeautifulSoup(wiki_file.read()).find(attrs={'id':'mw-content-text'})\n",
    "    \n",
    "    # The text is litered by references like [n].  Drop them.\n",
    "    def drop_refs(s):\n",
    "        return ''.join( re.split('\\[\\d+\\]', s) )\n",
    "    \n",
    "    paragraphs = [drop_refs(p.text) for p in soup.find_all('p')]\n",
    "    \n",
    "    raw_sents = reduce(lambda x, y: x + y, [nltk.tokenize.sent_tokenize(p.strip()) for p in paragraphs if p.strip()!=''])\n",
    "    return filter(lambda s: len(s.split(\" \"))>2, raw_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### Bag-of-words features, using tf-idf\n",
    "def make_ng_stem_vectorizer(texts, extra_stop_words, max_features):\n",
    "    \"\"\"\n",
    "    Given \n",
    "        - a list of texts (\"documents\");\n",
    "        - a list of extra stop words (in addition to the standard NLTK English ones); and,\n",
    "        - a number of features to remember\n",
    "    Returns the tf-idf feature extractor with this number of features, based on these documents.\n",
    "    \"\"\"\n",
    "    default_tokenizer = TfidfVectorizer().build_tokenizer()\n",
    "    stemmer = nltk.stem.SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "    def tokenize_stem(text):\n",
    "        return map(stemmer.stem, default_tokenizer(text))\n",
    "    ng_stem_tfidf=TfidfVectorizer(#max_features=max_features, \n",
    "                             ngram_range=(1,2),\n",
    "                             stop_words=map(stemmer.stem, nltk.corpus.stopwords.words('english') + extra_stop_words),\n",
    "                             tokenizer = tokenize_stem)\n",
    "    return ng_stem_tfidf.fit( texts )\n",
    "\n",
    "\n",
    "#### Ad hoc features\n",
    "def feature_verbs(words, positions):\n",
    "    pos_tag = nltk.pos_tag(words)\n",
    "    return len( [ i for i in positions if pos_tag[i][1] == 'VB'] )\n",
    "\n",
    "def is_cap(word):\n",
    "    return word[0] == word[0].capitalize()\n",
    "def feature_caps(words, positions):\n",
    "    return len ( [ i for i in positions if is_cap(words[i]) ] )\n",
    "\n",
    "def feature_plural(words, positions):\n",
    "    def is_plural(word):\n",
    "        return re.match( \".*s$\", word )\n",
    "    return len ( [ i for i in positions if is_plural(words[i]) ] )\n",
    "\n",
    "## N.B. The nltk word tokenizer will tokenize \"Apple's\" as [\"Apple\", \"'s\"]\n",
    "def feature_posessive(words, positions):\n",
    "    l = len(words)\n",
    "    return len ( [ i for i in positions if i+1 < l and words[i+1]==\"'s\" ] )\n",
    "\n",
    "def ad_hoc_features(keyword, strs, use_verbs=False):\n",
    "    \"\"\"\n",
    "    Given a keyword (e.g., \"apple\") and a list of strings;\n",
    "    Returns a numpy ndarray encoding several ad hoc features of the string that are local\n",
    "    near occurances of the keyword:\n",
    "        - If the keyword is capitalized\n",
    "        - If it is plural (in the stupid sense of ending in s.. good enough for 'apple')\n",
    "        - If it is possessive in the stupid sense of being followed by 's)\n",
    "        - If the keyword is a verb (e.g., for Ford vs ford)\n",
    "    \"\"\"\n",
    "    stemmed_word = stemmer.stem(keyword)\n",
    "    def feature_one(s):\n",
    "        words = nltk.tokenize.word_tokenize(s)\n",
    "        hits = [ i for i in range(len(words)) if stemmer.stem(words[i]) == stemmed_word ]\n",
    "        ret_list = [ feature_caps(words, hits), \n",
    "                     feature_plural(words, hits), \n",
    "                     feature_posessive(words, hits) ]\n",
    "        if use_verbs:  # This is slow, so only use it sometimes\n",
    "            ret_list.append( feature_verbs(words, hits)  )\n",
    "        return np.array(ret_list)\n",
    "    return np.asarray(map(feature_one, strs))\n",
    "\n",
    "####\n",
    "def make_classifier(base_word, meaning1, meaning2, use_verbs=False):\n",
    "    \"\"\"\n",
    "    Given\n",
    "        - a base word (e.g., \"apple\", \"ford\") that can have ambiguous meaning\n",
    "        - a pair meaning1 = (name1, url1) of a label for the first meaning, and a Wikipedia URL for it\n",
    "        - a pair meaning2 = ... for the other meaning\n",
    "    Returns a tuple (make_features, classifier) where\n",
    "        - make_features is a function taking in a string text, and returns a feature vector\n",
    "        - classifier takes in a feature vector (output by make_features) and predicts the meaning\n",
    "    \"\"\"\n",
    "    name1, url1 = meaning1\n",
    "    name2, url2 = meaning2\n",
    "    sents1 = wikipedia_to_sents(url1)\n",
    "    sents2 = wikipedia_to_sents(url2)\n",
    "    tfidf_vect = make_ng_stem_vectorizer(sents1 + sents2,\n",
    "                                        [base_word],\n",
    "                                        100000)\n",
    "    def make_features(sents, use_verbs=False):\n",
    "        a = ad_hoc_features(base_word, sents, use_verbs)\n",
    "        t = tfidf_vect.transform(sents).toarray()\n",
    "        return np.hstack((a,t))\n",
    "\n",
    "    # Build the training data\n",
    "    train_feat = make_features(sents1 + sents2, use_verbs)\n",
    "    train_res  = np.array( [0] * len(sents1) + [1] * len(sents2) )\n",
    "    \n",
    "    classifier = MultinomialNB()\n",
    "    classifier = classifier.fit(train_feat, train_res)\n",
    "    return (lambda x: make_features(x, use_verbs), classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fruit', 'company', 'company', 'company', 'fruit', 'fruit', 'company', 'fruit']\n"
     ]
    }
   ],
   "source": [
    "#### Now we actually run our code for Apple\n",
    "base_word=\"apple\"\n",
    "options = [ (\"fruit\", \"http://en.wikipedia.org/wiki/Apple\"),\n",
    "            (\"company\", \"http://en.wikipedia.org/wiki/Apple_Inc.\") ]\n",
    "(make_features, classifier) = make_classifier(\"apple\", *options)\n",
    "\n",
    "print map(lambda x: options[x][0], classifier.predict(make_features([\n",
    "    \"I'm baking a pie with my granny smith apples.\",\n",
    "    \"I looked up the recipe on my Apple iPhone.\",\n",
    "    \"The apple pie recipe is on my desk.\",\n",
    "    \"How is Apple's stock doing?\",\n",
    "    \"I'm drinking apple juice.\",\n",
    "    \"I have three apples.\",\n",
    "    \"Steve Jobs is the CEO of apple.\",\n",
    "    \"Steve Jobs likes to eat apples.\"\n",
    "])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['software', 'building', 'software', 'building', 'software', 'software']\n"
     ]
    }
   ],
   "source": [
    "#\"\"\" Uncomment to try this on your own\n",
    "#### Now we actually run our code for Apple\n",
    "base_word=\"windows\"\n",
    "options = [ (\"building\", \"http://en.wikipedia.org/wiki/Window\"),\n",
    "            (\"software\", \"http://en.wikipedia.org/wiki/Microsoft_Windows\") ]\n",
    "(make_features, classifier) = make_classifier(\"windows\", use_verbs=True, *options)\n",
    "print map(lambda x: options[x][0], classifier.predict(make_features([\n",
    "    \"Bill Gates was involved with Windows.\",\n",
    "    \"Could you open the window?\",\n",
    "    \"The 'broken window' theory related broken windows to increases in crime rate.\",\n",
    "    \"The windows are all made of shatter-proof glass.\",\n",
    "    \"Could you install windows on your computer?\",\n",
    "    \"Could you install windows on your house?\"\n",
    "])))\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['company', 'company', 'company', 'company']\n"
     ]
    }
   ],
   "source": [
    "#\"\"\" Uncomment to try this on your own\n",
    "#### Now we actually run our code for Ford\n",
    "base_word=\"ford\"\n",
    "options = [ (\"crossing\", \"http://en.wikipedia.org/wiki/Ford_(crossing)\"),\n",
    "            (\"company\", \"http://en.wikipedia.org/wiki/Ford\") ]\n",
    "(make_features, classifier) = make_classifier(\"ford\", *options)\n",
    "print map(lambda x: options[x][0], classifier.predict(make_features([\n",
    "    \"I tried to ford the river and my unfortunate oxen died.\",\n",
    "    \"Ford makes cars, though their quality is sometimes in dispute.\",\n",
    "    \"The Ford Mustang is an iconic automobile.\",\n",
    "    \"The river crossing was shallow, but we could not ford it.\"\n",
    "])))\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises / Brainstorming for Improvement:\n",
    "\n",
    "1. Change the code to use just the ad hoc features.  How does this change the results?  Why do you think this is?\n",
    "\n",
    "2. Same question as 1, but for the tf-idf features.\n",
    "\n",
    "3. Change the formation of tf-idf features as follows -- when doing the tf-idf weighting (in the call to fit appearing in make_ng_stem_vectorizer) we pass in the sentences as separate documents.  How do the results change if we pass them in as just two documents?\n",
    "\n",
    "4. What ideas do you think could improve the performance of this model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exit Tickets\n",
    "1. What are some other options for modeling with text data besides bag of words?\n",
    "1. How would you account for the fact that word meanings change over time?\n",
    "1. How do stopwords, stemming, and limiting the # features affect variance-bias?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spoilers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some ideas / hints for the exercises:\n",
    "  - A key problem with the model is the small amount of training data.  At the least, we could follow links from the given Wikipedia articles.  Better would be to find other sources that directly use the words Apple/apple.\n",
    "  - In this specific case (apple/Apple) we would do better by using a few human created absolute rules _first_: e.g., typos aside -- apple's will always refer to the company and apples to the fruit, so we do not need to run a more complicated learner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional NLP topics and resources\n",
    "\n",
    "Natural language processing is a big field.  We only (really) talked about a few tools and techniques.  Here are some other terms that are relevant:\n",
    "\n",
    " - Context free grammars (and probabilistic context free grammars): This is a simple and basic technique for parsing.  \n",
    " - [Word2vec](https://code.google.com/p/word2vec/) is a popular tool for creating a vectorized representation of a text corpus. The learned vectors can then be used to identify/predict words similar to a target, or even (weakly) to reason by analogy. For example, vector('Paris') - vector('France') + vector('Italy') results in a vector that is very close to vector('Rome'), and vector('king') - vector('man') + vector('woman') is close to vector('queen').\n",
    " \n",
    " To use Word2vec in Python (and get the computation speed improvements) look at [gensim](https://radimrehurek.com/gensim/models/word2vec.html) and [cython](http://docs.cython.org/src/quickstart/install.html). This [Git repo](https://github.com/danielfrg/word2vec) is an alternative way to access the algorithm. You might also use this [Kaggle competition](https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-2-word-vectors) as a reference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provisio (old)\n",
    "Before running some of what's below, you will want to _disable_ `pylab` (which is enabled by default in the `ipython notebook` configuration I set up for our DO instances).   The easiest way to do this is to run the following code (or just change the indicated line in the configuration file) , and then to restart ipython notebook:\n",
    "\n",
    "```bash\n",
    "patch ~/.ipython/profile_nbserver/ipython_notebook_config.py <<EOF\n",
    "643c643\n",
    "< c.IPKernelApp.pylab = 'inline'\n",
    "---\n",
    "> c.IPKernelApp.matplotlib = 'inline'\n",
    "EOF\n",
    "```\n",
    "\n",
    "(we talked about this in lecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2015 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
